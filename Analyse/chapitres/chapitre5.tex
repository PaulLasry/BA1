% !TEX root = ../Analyse1.tex
\chapter{Dériviées}
\section{Définitions et exemples}
\begin{definition}[Dériviée]
    Soit $f: D\to\R$ définie au voisinage $x_0$ ou en $x_0$. Alors $f$ est $\underline{\text{dérivable}}$  ou $\underline{\text{différentiable}}$ en $x_0$ si
    la limite
    \[f'(x_0)=\lim_{h\to 0}\frac{f(x_0+h) -f(x_0)}{h}\]
    existe ($\in \R$).\\
    \textbf{Notation :} \[f'(x_0) = \frac{df}{dx}(x_0) = \partial_xf(x_0) =\mathcal{D}_x f(x_0) = \dot{f}(x_0)\]
    On dit :\begin{itemize}
        \item $f'(x_0)$ est la dérivée de $f$ en $x_0$\\
        \item $f:D\to\R$ est $\underline{\text{dérivable}}$ si elle est dérivable en tout $x_0 \in D$.
    \end{itemize}
\end{definition}
\begin{remark}
    Le nombre $f'(x_0)$ est la pentes de la tengente à la courbe $y=f(x)$ au point$(x_0, f(x_0))$.
\end{remark}
\begin{example}
    \begin{align*}
        f'(x_0)&=\lim_{h\to 0}\frac{f(x_0+h) -f(x_0)}{h}\\
        &=\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}
    \end{align*}
\end{example}
\begin{definition}[La fonction dérivée]
    La $\underline{\text{fonction dérivée }}$ d'une fonction $f:D\to\R$ est la fonction \(\begin{aligned}[t]
        f':D(f(x))&\longrightarrow\R\\
        x&\longmapsto f'(x)
    \end{aligned}\) où $D(f') = \left\{x\in D | f\text{ est dérivable en } x\right\}$
\end{definition}
\begin{example}[1]
    $f(x) = x^2$. On a \begin{align*}
        f'(x_0) &=\lim_{h\to 0}\frac{f(x_0+h) -f(x_0)}{h}\\
        &=\lim_{h\to 0}\frac{(x_0+h)^2 -x_0^2}{h}\\
        &= \lim_{h\to 0}(2x_0 +h) = 2\cdot x_0.
    \end{align*}
    Ainsi, $f(x)=x^2$ est dérivable pour tout $x_0\in\R$. Sa dérivée est $f'(x) = 2x$.
\end{example}
\begin{example}[2]
    $f(x)=\sin(x)$, $x_0\in \R$. On a\begin{align*}
        f'(x_0) =& \lim_{h\to 0}\frac{\sin(x_0+h)-\sin(x_0)}{h}\\ 
        =&\lim_{h\to 0}\frac{\sin(x_0)\cos(h)+\cos(x_0)\sin(h)-\sin(x_0)}{h}\\ 
        =& \sin(x_0)\cdot\lim_{h\to 0} \frac{\cos(h)-1}{h} + \cos(x_0)\cdot\lim_{h\to 0}\frac{\sin(h)}{h}\\ 
        \implies& \underbrace{-h}_{\to 0} = \frac{1-h^2-1}{h}\geq\underbrace{\frac{\cos(h)-1}{h}}_{\to 0}\geq \frac{0}{h} = \underbrace{0}_{\to 0}\\
        \implies& \sin(x_0)\cdot 0+\cos(x_0)\cdot1 = \cos(x_0)
    \end{align*}
    $\sin$ est dérivable sur $\R$ et $\sin'(x) = \cos(x)$. De manière analogue : $\cos'(x) = -\sin(x)$²
\end{example}
\begin{proposition}
    Soit $f:D\to\R$
    \begin{enumerate}
        \item Si $f$ est dérivable en $x_0$, alors $f$ est aussi continue en $x_0$
        \item $f$ est dérivable en $x_0$ si et seulement si :\[f(x) = \underbrace{f(x_0)+f'(x_0)\cdot(x-x_0)}_{\text{équation de la tengente}}+ \underbrace{\textcolor{red}{(x-x_0)\cdot\varepsilon(x)}}_{\text{reste}}\]
            où $\varepsilon(x)$ est une fonction tel que $\lim_{x\to x_0} \varepsilon(x)=0$. Le \textcolor{red}{reste} tend plus vite vers $0$ que $x-x_0$
    \end{enumerate}
\end{proposition}
\begin{proof}[Preuve de la proposition]
    \noindent
    \begin{enumerate}
        \item On a \[\lim_{x\to x_0} f(x) = f(x_0) + \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} \cdot (x-x_0) = f(x_0)-f'(x_0)\cdot0 = f(x_0)\]
        \item Poser $\varepsilon(x)=\frac{f(x)-f(x_0)-f'(x-0)(x-x_0)}{x-x_0}$ et calculer la limite lorsque $x\to x_0$
    \end{enumerate}
\end{proof}
\begin{remark}
    $f$ continues $\not \implies$ $f$ dérivable.
\end{remark}
\begin{example}
    $f(x) = |x|$, $x_0=0$. Alors $f$ est continue en $0$ mais on a \[\lim_{h\downarrow0}\frac{f(0+h)-f(0)}{h} = \lim_{h\downarrow0}\frac{|h|}{h} = 1\neq -1 = \lim_{h\uparrow0}\frac{|h|}{h} = \lim_{h\uparrow0}\frac{f(0+h)-f(0)}{h}\]
    Donc la limite n'existe pas, donc $f$ n'est pas dérivable en $0$.
\end{example}
\begin{property}[Opérations algébriques]
    Soient $f, g : D\to\R$ derivables en $x_0\in D$. Alors :
    \begin{enumerate}
        \item $\displaystyle(p\cdot f+q\cdot g)'(x_0) = p\cdot f'(x_0)+q\cdot g'(x_0)$
        \item $\displaystyle(f\cdot g)'(x_0) = f'(x_0)\cdot g(x_0) + f(x_0)\cdot g'(x_0)$
        \item $\displaystyle\left(\frac{f}{g}\right)'(x_0) = \frac{f'(x_0)\cdot g(x_0) - f(x_0)\cdot g'(x_0)}{(g(x_0))^2}$, si $g(x_0)\neq 0$
    \end{enumerate}
\end{property}
\begin{property}[Dérivées de fonction usuelles]
    \noindent
    \begin{enumerate}[start=0]
        \item $f(x) = c\in \R \implies f'(x) = 0$
        \item $f(x) = x^n \implies f'(x) = n\cdot x^{n-1}$ pour $\N^*$
        \begin{proof}[Preuve]
            Par récurrence sur $n$.
            \begin{itemize}
                \item $n=1$: $f(x) = x$, $f'(x) = 1$.
                \item Supposons vrai pour $n$, montrons pour $n+1$ :
                    \begin{align*}
                        f(x) &= x^{n+1} = x^n \cdot x\\
                        f'(x) &\overset{\text{Prop 2}}{=} (x^n)' \cdot x + x^n \cdot (x)'\\
                        &= n\cdot x^{n-1} \cdot x + x^n \cdot 1 = (n+1)\cdot x^n
                    \end{align*}
            \end{itemize}   
        \end{proof}
        \item $\sin'(x) = \cos(x), \cos'(x) = -\sin(x)$ et \[\tan'(x) = \left(\frac{\sin(x)}{\cos(x)}\right)' \overset{\text{Prop 3}}{=} \frac{\sin'(x)\cos - \sin(x)\cos'(x)}{\cos^2(x)} = \cdots = \frac{1}{\cos^2(x)} \text{ ou } 1 + \tan^2(x)\]
        \item $f(x)=x^{-n}$, $n\in \N^*, x\neq0$\[\implies f(x)=\frac{1}{x^n} \overset{\text{Prop 3}}{\implies} f'(x) = -n\cdot x^{-n-1}\]
    \end{enumerate}
\end{property}
\begin{property}[Dériviées de composées]
    Soient $f : A\to B$, $g: B \to \R$, deux fonction telles que $f$ est dérivable en $x_0$ et $g$ est dérivable en $f(x_0)$. Alors:
    \begin{align*}
        \left(g(f(x_0))\right)' &= (g\circ f)'(x_0) \\
        &= g'(f(x_0))\cdot \underbrace{f'(x_0)}_{\text{Dériviée interne}}
    \end{align*}
    \begin{proof}[Preuve]
        Le quotient est \begin{align*}
            &\frac{g(f(x_0+h)) - g(f(x_0))}{h} \\
            =& \frac{g(f(x_0+h)) - g(f(x_0))}{f(x_0+h) - f(x_0)} \cdot \frac{f(x_0+h) - f(x_0)}{h}
        \end{align*}
        En prenant la limite lorsque $h\to 0$, on obtient le résultat voulu.
    \end{proof}
\end{property}
\begin{property}[Dériviées des réciproques]
    Soit $f: A\to B$ bijective et dérivable sur $A$, un intervalle ouvert. Si $f'(x)\neq 0$ pour tout $x\in A$, alors
    \[\left(f^{-1}\right)'(x) = \frac{1}{f'(f^{-1(x)})}\]
    \begin{proof}[Preuve]
        On admet que $f^{-1}$ est dérivable sur tout $B$. On dérive l'equation $f(f^{-1}(x)) = x$ des deux cotés :
        \begin{align*} 1 &= (f\circ f^{-1})'(x) = f'\left(f^{-1}(x)\right)\\ \implies& \frac{1}{f'\left(f^{-1}(x)\right)} = (f^{-1})'(x) \end{align*} \end{proof} \end{property} \begin{example}[1] $f(x) = \sqrt[n]{x} = f^{-1}(x)$ où $f(x)=x^n$, où $x>0$ \begin{align*} x &= f(g(x)) = \left(\sqrt[n]{x}\right)^n\\
        &\implies 1 = n(\sqrt[n]{x})^{n-1}\cdot (\sqrt[n]{x})'\\
        &\implies (\sqrt[n]{x})' = \frac{1}{n(\sqrt[n]{x})^{n-1}} = \frac{1}{n}x^{\frac1n -1}
    \end{align*} 
    On montre de manière analogue que $\left(x^{\frac{p}{q}}\right) = \frac{p}{q}x^{\frac{p}{q}-1}$ pour tout $\frac{p}{q} \in \Q$ et on verra que c'est aussi vrai pour tout réel.
\end{example}
\begin{example}[2]
    $\arcsin'(x) = \frac{1}{\cos(\arcsin(x))}, x\in ]-1,1[$
        \begin{center}
            \includegraphics[width=0.8\textwidth]{derivee_arcsin.png}
        \end{center}        
    \[\implies arcsin'(x) = \frac{1}{\sqrt{1-x^2}}\]
\end{example}
\begin{definition}
    $\lim_{h \downarrow 0}\frac{f(x+h)-f(h)}{h}$ est la dérivée à droite, si $h\uparrow 0$, c'est la dérivée à gauche.
\end{definition}
\begin{property}
    $f$ est dérivable en $x_0$ si et seulement si les deux dérivées latérales existent et sont égales.
\end{property}
\begin{example}[1]
    $f(x) = \left|x\right|$. On a:
    \begin{align*}
        f'_{\text{doite}} (0) = 1 \neq -1 = f'_{\text{gauche}}(0)
    \end{align*} 
    Donc $f$ n'est pas dérivable en $0$.
\end{example}
\begin{example}[2]
    $f(x) = \sqrt[3]{x}$. On a:
    \[\lim_{h\to 0}\frac{f(0+h)-f(0)}{h} = \lim_{x\to 0}\frac{\sqrt[3]{h}}{h} = \lim_{h \to 0}\frac{1}{\left(\sqrt[3]{n}\right)^2} = \frac{1}{0^+} = +\infty\]
    $f'(0)$ n'existe pas, mais les deux dérivées latérales sont égales à $+\infty$.
\end{example}
\begin{definition}[Dériviée d'ordre supérieur]
    La \underline{dérivée seconde} de $f$ est \[f''(x)=f^{(2)}(x)=\left(f'(x)\right)'\]
    La \underline{dérivée d'ordre n} de $f$ est définie par récurrence : \[f^{(n)}(x) = \left(f^{(n-1)}(x)\right)'\]
\end{definition}
\begin{definition}[Clsses de fonctions dérivables]
    Soit $I$ un intervalle. Alors
    \begin{align*}
        &D^n(I) = \left\{f : I\to \R | \,f \text{ est $n$ fois dérivable sur } I\right\}\\
        &C^n(I) = \left\{f : I\to \R | \,f \text{ est $n$ fois dérivable sur et $f^{(n)}$ est continue sur } I\right\}
    \end{align*}
    On pose $C^{\infty}(I) = \left\{f : I\to\R\, |\, f^{(n)}\text{ existe pour tour } n\in \N\right\}$.
\end{definition}
\begin{remark}
    \noindent
    \begin{itemize}
        \item $C^0(I)$ est l'ensemble des fonctions continues sur $I$.
        \item Les fonction polynomiales, trigonométriques, exponentielles, logarithmes, rationnelles sont dans $C^{\infty}$ sur leur domaine de définition.
        \item $C^0(I) \supseteq D^1(I)\supseteq C^1(I) \supseteq D^2(I) \supseteq C^2(I) \supseteq \cdots D^{\infty} \supseteq C^{\infty}$
    \end{itemize}
\end{remark}
\begin{center}
    \includegraphics[width=0.8\textwidth]{classes_derivee.png}

\end{center}
\begin{remark}
    \noindent
    \begin{itemize}
        \item $x^n|x|$ est dans $C^{n}(\R)\setminus D^{n+1}(\R)$
        \item $x|x| \begin{aligned}[t]
            &\in C^{\infty}(]0, +\infty[)\\
            &\in C^{\infty}(]-\infty, 0[)
        \end{aligned}$
    \end{itemize}
\end{remark}
\begin{example}[$D^1\setminus C^1$]
    On a $f(x)=\begin{cases}
        x^2\cos\left(\frac{1}{x}\right) & x\neq 0\\
        0 & x=0
    \end{cases}$. Alors \[f'(x) = 2x\cos\left(\frac1x\right)+\sin\left(\frac{1}{x}\right) \; \text{si x $\neq 0$}\]
    et \[f'(0) = \lim_{h\to 0}\frac{f(h)-f(0)}{h} = \lim_{h\to 0} h\cos\left(\frac1h\right) = 0\]
    donc $f(x)$ appartient à $D^1(\R)$. Cependant, \[\lim_{x\to 0} f'(x) \text{ n'existe pas}\] donc $f'(x)$ n'est pas continue en $0$. Donc $f(x)\not\in C^1(\R)$.
\end{example}

\section{Dérivée et croissance}
\begin{theorem}[Théorhème de Rolle]
    Soit $f:[a,b]\to\R$ continue sur $[a,b]$ et dérivable sur $]a,b[$. On suppose que $f(a)=0=f(b)$. Alors $\exists u\in ]a, b[$ tel que $f'(u)=0$.
        \centering
        \includegraphics[width=0.5\textwidth]{rolle.png}    
\end{theorem}
\begin{proof}[Preuve par le TVI]
    Par le TVI, $f$ atteint son max $M=\max_{x\in[a,b]} f(x)$, on le suppose supérieur à $0$. Alors $\exists u\in ]a, b[$ tel que :
    \begin{align*}
        f(u)=M \implies f'(u)=f'_{\text{droite}}(u)&=\lim_{x\to u^+}\frac{f(x)-f(u)}{x-u}\leq 0\\ 
        \text{et}\quad f'(u) = f'_{\text{gauche}}(u) &= \lim_{x\to u^-}\frac{f(x)-f(u)}{x-u} \geq 0\\
    \end{align*}
    Donc $f'(u)=0$.
\end{proof}
\begin{theorem}[Théorhème des acroissements finis (TAF)]
    Soit $f:[a, b]\to \R$ continue, dérivable sur $]a, b[$. Alors $\exists u\in ]a, b[$ tel que :
    \begin{align*}
        f'(u)=\frac{f(b)-f(a)}{b-a}
    \end{align*}
    \centering
    \includegraphics[width=0.5\textwidth]{TAF.png}
\end{theorem}
\begin{proof}[Preuve]
    En exercice.
\end{proof}
\begin{property}[Application du TAF]
    Soit $f:[a,b]\to\R$ continue sur $[a,b]$ et dérivable sur $]a,b[$.
    \begin{enumerate}
        \item Si $f'(x)=0$ pour tout $x\in ]a,b[$, alors $f$ est constante sur $[a,b]$.
        \begin{proof}[Preuve]
            Le sens "$\impliedby$" est évident. Pour le sens "$\implies$", si $f$ n'est pas constante, on trouve $c, d \in [a, b]$ tels que $c<d$ et  $f(c) \neq f(d)$. 
            Par le TAF, $\exists u\in ]c,d[$ tel que \[f'(u) = \frac{f(d)-f(c)}{d-c} \neq 0\] ce qui est une contradiction.
        \end{proof}
        \item Si $g:[a, b]\overset{\text{continue}}{\longrightarrow }\R$ dérivable sur $]a,b[$ et $f'(x)=g'(x)$ pour tout $x\in ]a,b[$, alors $f(x) = g(x) + c$ pour un certain $c\in \R$ et tout $x\in [a,b]$.
        \item Si $f'(x)\geq 0$ pour tout $x\in ]a,b[$, alors $f$ est croissante sur $[a,b]$.
        \item Si $f'(x)\leq 0$ pour tout $x\in ]a,b[$, alors $f$ est décroissante sur $[a,b]$.
        \item Si $f'(x)>0$ pour tout $x\in ]a,b[$, alors $f$ est strictement croissante sur $[a,b]$.
        \item Si $f'(x)<0$ pour tout $x\in ]a,b[$, alors $f$ est strictement décroissante sur $[a,b]$.
    \end{enumerate}
\end{property}
\begin{remark}
    Attention au point 5 et 6, la réciproques est en général fausse. Par exemple, $f(x) = x^3$ est strictement croissante sur $\R$ mais $f'(0)=0$.
\end{remark}
\subsection{La fonction exponentielle (et logarithmes)}
\begin{theorem}
    Il existe une \underline{unique} fonction $f:\R\to\R$ telle que :
    \begin{align*}
        f'(x) = f(x) \; \forall x\in\R \quad \text{ et } \quad f(0) = 1 
    \end{align*}
\end{theorem}
\begin{proof}[Preuve]
    La preuve de l'existence se fera plus tard.\\ 
    Pour l'unicité, la preuve se fait en deux étapes :
    \begin{enumerate}
        \item $f(x)\neq0 \;\forall x\in \R$. On pose $h(x)=f(x)\cdot f(-x)$. On calcule la dérivée de $h$:
            \begin{align*}
                h'(x) &= f'(x)\cdot f(-x)+ f(x)\cdot \underbrace{f'(-x)}_{=f(-x)}(-1)\\
                &= 0
            \end{align*}
            Donc $h$ est constante ! Comme $h(0) = f(0)\cdot f(0) = 1$, on a \[h(x) = \underbrace{f(x)}_{\neq 0}\cdot f(-x) = 1\]
        \item \textbf{Unicité : } Soit $g(x)$ une (autre) fonction telle que $g'(x)=g(x)$ et $g(0)=1$. On pose $h(x) = \frac{g(x)}{f(x)}$. On calcule $h'(x)$ :
            \begin{align*}
                h'(x) = \frac{g'\cdot f - g\cdot f'}{f^2} = \frac{g\cdot f - g\cdot f}{f^2} = 0
            \end{align*}
            Donc $h$ est constante. Comme $h(0) = \frac11 = 1$ on a que $h(x)=1=\frac{g(x)}{f(x)} = 1 \implies g(x) = f(x)$ 
    \end{enumerate} 
\end{proof}

\begin{definition}[Foonction exponentielle]
    Cette fonction s'appelle la fonction \\ \underline{exponentielle}. Notée \[\exp(x) \quad \text{ (plus tard } e^x)\]
\end{definition}
\begin{property}[$\exp(x)$]
    \noindent
    \begin{enumerate}
        \item $\exp'(x) = \exp(x)$ et $\exp(0)=1$ nous donne \[\exp \in C^\infty(\R)\]
        \item $\exp(x)\neq 0 \; \forall x\in \R$ et $\exp(-x) = \displaystyle\frac{1}{\exp(x)}$
        \item $\exp$ est strictement croissante
        \item $\exp(x+y) = \exp(x)\cdot\exp(y) \; \forall x\in\R$
        \item $\lim_{x\to +\infty} \exp(x)=+\infty$, $\lim_{x\to 0} \exp(x)=0$
            \begin{proof}[Preuve]
                $\exp(x) \geq x$. En effet, si $g(x) = \exp(x)-x$, on a 
                \begin{align*}
                    g'(x) =& \exp(x)-1 > 0 (x > 0)\\
                    \implies& g \text{ est strictement croissante sur } [0, +\infty[
                \end{align*}  
                Comme $g(0)=1$, donc $g(x)\geq 0$
            \end{proof}
        \item $\exp(1) =\displaystyle \lim_{n\to \infty}\left(1+\frac{1}{n}\right)^n$
    \end{enumerate}
    Ainsi : $\exp(2) = \exp(1+1) = \exp(1)\cdot\exp(1) = e^2$ et 
    \[\exp(n) = e^n \quad \forall x\in\R\]
    De plus $\exp(-n) = \frac{1}{\exp(n)} = \frac{1}{e^n} = e^{-n}$ et on vérifie que $\left(1+\frac{1}{n}\right)^n = e$. Donc :
    \[\exp\left(\frac{p}{q} = e^{\frac{p}{q}}\right) \quad \forall \textstyle\frac{p}{q}\in\Q\]
\end{property}
\begin{definition}
    Pour $x\in\R$, on pose que \[e^x = \exp(x)\]
\end{definition}
\begin{remark}
    La fonction exponentielle est 
    \begin{itemize}
        \item injective (car strictement croissante)
        \item surjective (coréstraintes à $]0, +\infty[$)
    \end{itemize}
    Donc $\exp(x)$ est bijective !
\end{remark}
\begin{definition}[Logarithme]
    Le \underline{logarithme} est la réciproque que la fonction exponentielle. Donc :
    \begin{align*}
        \log : \; ]0, + \infty[ &\longrightarrow \R\\
        x &\longmapsto \log(x) = \ln(x)
    \end{align*}
\end{definition}
\begin{property}[Logarithme]
    \noindent
    \begin{enumerate}
        \item $\mathcal{D}(\ln) = \;]0, + \infty[$ et $\Im(\ln) = \R$. 
        \item $\ln(1) = 0$. On a :
            \begin{align*}
                &x = \exp(\ln(x)) \\
                \iff& 1 = \exp'(\ln(x))\cdot\ln'(x) = x\ln'(x)\\
                \implies& \ln'(x) = \frac{1}{x}\\
                \implies& \ln \in C^\infty\left(]0, +\infty[\right)
            \end{align*} 
        
        \item $\log$ est strictement croissante.
        \item $\lim_{x\to+\infty}\log(x)=+\infty$ et $\lim_{x\to 0^+}\log(x)=-\infty$.
    \end{enumerate}
\end{property}
\begin{definition}[Autres bases]
    Pour $a\in\R > 0$\begin{align*}
        \exp_a(x) &\overset{\text{def}}{=} \exp(\log(a)\cdot x) = a^x\\ 
        \log_a(x) &= \text{ réciproque }\\ 
        &= \frac{\ln(x)}{\ln(a)}
    \end{align*}
\end{definition}
\begin{remark}
    Pour $x, u\in\R$ et $x>0$, on a :
    \begin{align*}
        x^u &= \exp(\log(x)\cdot u)\\
        \implies (x^u)'&= \exp(\log(x)\cdot u)\cdot(\log(x)\cdot u)' = ux^{u-1}
    \end{align*} 
\end{remark}
\begin{definition}[Fonctions trigonométriques hyperboliques]
    On défini :
    \begin{align*}
        \sinh(x)&=\frac{e^x-e^{-x}}{2},\\
        \cosh(x)&=\frac{e^x+e^{-x}}{2},\\
        \tanh(x)&=\frac{\sinh(x)}{\cosh(x)} 
    \end{align*}
\end{definition}
\begin{theorem}[Règle de Bernouilli-l'Hospital]
    Soit $x_0$ et $f, g: A\to\R$, où $A=]x_0-d, \, x_0+d[\setminus{\{x_0\}}$ est un voisinage de $x_0$. Si :
    \begin{enumerate}
        \item $\displaystyle \frac{\lim_{x\to x_0}f(x)}{\lim_{x\to x_0}g(x)} = \frac{0}{0} \text{ ou } \frac{\infty}{\infty}$
        \item $\displaystyle \frac{f'(x)}{g'(x)} = l \in\R\cup \{\pm \infty\}$
    \end{enumerate}
    Alors $\lim_{x\to x_0} \frac{f(x)}{g(x)} = \lim_{x\to x_0}\frac{f'(x)}{g'(x)}$
\end{theorem}
\begin{proof}[Preuve]
    En exercices (Application du TAF)
\end{proof}
\begin{remark}
    Ce théorème marche aussi pour limite à droite et à gauche.
\end{remark}
\begin{example}[1]
    Grâce a l'Hostpital, on a : \[\lim_{x\to 0}\frac{\sin(x)}{x} \overset{\text{B-H}}{=} \lim_{x\to 0} \frac{\cos(x)}{x} = 1\]
\end{example}
\begin{example}[2]
    \begin{align*}
        \lim_{x\to +\infty} \frac{x^p}{\log(x)} &\overset{\text{B-H}}{=} \lim_{x\to + \infty} \frac{px^{p-1}}{\frac{1}{x}}\\
        &= \lim_{x\to +\infty} px^p = \begin{cases}
            +\infty & p>0\\
            0 & p \leq 0
        \end{cases}
    \end{align*}
    Cela montre que $\log(x)$ croît moins vite que tout polynôme.
\end{example}
\begin{example}[3]
    \begin{align*}
        \lim_{x\to 0}\cos(2x)^{\frac{3}{x^2}} &= \lim_{x\to 0}\exp(\frac{\log(\cos(2x)\cdot3)}{x^2})\\ 
        &= \exp\left(\lim_{x\to 0}\left(\frac{3}{\cos(2x)}\cdot\frac{-\sin(2x)}{2x}\cdot2\right)\right)\\
        &= \exp\left(\frac{3}{\cos(0)}\cdot\lim_{y\to 0} \frac{-\sin(y)}{y}\cdot2\right) \\
        &= \exp(-6) = e^{-6}
    \end{align*}
\end{example}
\begin{remark}
    \textbf{Attention :} Si la limite du quotient des dérivées n'existe pas ($\notin \R\cup\{\pm \infty\}$), alors B-H ne marche pas.
\end{remark}
\begin{property}[Finction $C^1$ par morceaux]
    Soient $f, g \in C^1(I = \text{intervalle})$ et $x_0\in I$. On suppose que $f(x_0)=g(x_0)$, et on pose \begin{align*}
        h : I&\to\R\\
        x&\mapsto\begin{cases}
            f(x)&x\leq x_0\\
            g(x)&x\geq x_0
        \end{cases}
    \end{align*}
    Alors $h$ est dérivable en $x_0$ si et seulement si $f'(x_0)=g'(x_0)$ et dans ce cas, $h\in C^1(I)$.
\end{property}
\begin{proof}[Preuve]
    On calcule \begin{align*}
        h'_{\text{gauche}}(x_0) &\overset{x<x_0}{=}\lim_{x\uparrow x_0}\frac{h(x)-h(x_0)}{x-x_0}\\
        &\overset{\text{def de } h}{=}\lim_{x\uparrow x_0}\frac{f(x)-f(x_0)}{x-x_0}\\
        &\overset{\text{B-H}}{=} \lim_{x\to x_0}\frac{f'(x)}{1} \\
        &= f'(x_0)
    \end{align*}
    Similaire pour $h'_{droite}(x_0) = g'(x_0)$. La limites à gauche et à droite coïncindent si et seulement si $f'(x_0)=g'(x_0)$. Dans ce cas, on a :
    \begin{align*}
        h'(x) = \begin{cases}
            f'(x) & x\leq 0\\
            g'(x) & x\geq 0
        \end{cases} \implies h' \text{ est continue en } x_0
    \end{align*}
    Dans ce cas, $h'$ est continue sur $I$ ($h\in C^1$)
\end{proof}
\begin{remark}
    Cette propriété peut être utilisée récursivement
    \begin{itemize}
        \item La preuve montre : $\begin{aligned}[t]
            &f'_{\text{gauche}}(x_0) = \lim_{x\to x_0^-}f'(x)\\
            &f'_{\text{droite}}(x_0) = \lim_{x\to x_0^+}f'(x)
        \end{aligned}$\\ si les limites existent
    \end{itemize}
\end{remark}
\begin{example}
    Soit $f(x) = \begin{cases}
        \sinh(x) &x\leq 0\\
        \sin(x)&x>0
    \end{cases}$.
    Alors: \begin{itemize}
        \item $f$ est continue en $0$ car $\sinh(0) = 0 = \sin(0)$
        \item de classe $C^1$ sur $\R$, car $\sinh'(0)=\cosh(0)=1$ et $sin'(0)=\cos(0)=1$
        \item $\in C^2$ car $\sinh''(0)=\sinh(0)=0 = -\sin(0)=\sin''(0)$
        \item $\notin C^3$ car $\sinh'''(0)=\cosh(0)=1 \neq \sin'''(0) = -1$
    \end{itemize}
    Donc $f(x) \in C^2(\R)\setminus D^3(\R)$
\end{example}

\section{Etude de fonctions}
Slides
\subsection{Applications : convergence de suites définies par récurrence}
\noindent\textbf{Rappel :} Une suite définie par récurrence est une suite $(a_n)_{n\in\N}$ définie par $a_0 = \text{valeur fixée}$ et $a_{n+1} = g(a_n)$, pour une fonction $g:D\to\R$.
\begin{remark}[Important]
    \textcolor{red}{Si} $(a_n)$ converge, disons $a_n\to l\in\R$, alors: \begin{align*}
        l &= \lim_{n\to\infty}a_{n+1} = \lim_{n\to\infty}g(a_n)\\
        &\overset{g\text{ est cont.}}{=}g(\lim_{n\to\infty} a_n) = g(l)
    \end{align*}
    Donc $l$ est foncément une sol de l'équation $\underline{x = g(x)}$. On supposera en général que $g(x)$ est continue et même $C^1$ sur un intervalle. 
\end{remark}
\begin{example}
    Soit $a_0 = \frac13$, $a_{n+1} = a_n^2 = g(a_n)$ où $g(x)=x^2$. Condidats pour $l$ : $x = g(x)=x^2$, donc $l = 0$ ou $l =1$. On calcule quelques valeurs :
    \begin{align*}
        a_0 = \frac13, a_1 = \frac{1}{9}, a_2 = \frac{1}{81}, \ldots \to 0
    \end{align*}
    En revanche, si $a_0 = 3$, on a :
    \begin{align*}
        (a_n) = (3, 9, 81, 81^2, \ldots)\to +\infty
    \end{align*}
    De plus, si $a_0=-1$, alors :
    \begin{align*}
        (a_n) = (-1, 1, 1, 1, \ldots)\to 1
    \end{align*}
    Dans un cas simple comme celui-là, on peut montrer par récurrence que :
    \begin{align*}
        a_n = (a_n)^2 \quad \forall n\in\N
    \end{align*}
    Donc :
    \[\begin{cases}
        a_n \to 0 & |a_0|<1\\
        a_n \to 1 & a_0 = \pm1\\
        a_n \to +\infty & |a_0| > 1
    \end{cases}\]
    Dans le cas général, une étude de la fonction $g(x)$ peut nous aider. Si \begin{align}\label{eq:recurrence_condition}
        a_0 \geq l \text{ et }l \leq g(x)\leq x \quad \forall x \geq l
    \end{align}
    alors:
    \begin{itemize}
        \item $a_1 = g(a_0) \geq l$, $a_2 = g(a_1) \geq l$. Par récurrence : $a_{n+1} =g(a_n) \geq l$, donc $a_n\geq l$ pour tout $n\in\N$ donc $(a_n)$ est \underline{minorée}.
        \item $a_1 = g(a_0) \leq a_0 \implies \ldots \implies a_{n+1} = g(a_n) \leq a_n$ et par récurrence, $(a_n)$ est \underline{décroissante}
    \end{itemize}
    Ainsi, dans ce as $(a_n)$ converge vers un candidat $l$ par décroissance minorée.
\end{example}
\begin{remark}
    Equation \ref{eq:recurrence_condition} se généralise
\end{remark}
\begin{theorem}[Récurrence linéaire]
    Soit $(a_n)$ définie par récurrence via \[a_0 = a, a_{n+1} = g(a_n)\] où $g(x) = qx+b$, où $q, b\in\R$ et $q\neq 1$. Alors $(a_n)$ converge vers l'unique solution $l$ de l'equation $g(x)=x$ si et seulement si \[|q|<1 \quad \text{ou}\quad a_0=l\]
    Donc \[l\leq g(x)\leq x\]
\end{theorem}
\begin{proof}
    \textbf{Illustration :}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{convergence_lin.png}
    \end{center}
    On voit que dans le cas $(1)$, la fonction converge vers le seul candidat : $l$. Dans le cas $(2)$, la fonction converge aussi. Dans le cas $(3)$, la fonction diverge.
    Si $q\leq0$, on pose \begin{align*}
        b_k &= a_{2k}\\
        c_k &= a_{2k+1}
    \end{align*}
    Ces suites sont définies par récurrence : $b_0 = a_0$, $b_{k+1}=a_{2k+2} = g(g(a_{2k})) = h(b_k)$ où $h(x)=g(g(x)) = g\circ g(x) = g(qx+b)= q^2x + qb+b$. Pareil pour $c_n$ :
    \begin{align*}
        c_0 = a_1, c_{n+1}=g(g(a_{2k+1})) = h(c_k)
    \end{align*} 
    Si $-1<q\leq0$, alors, $0\leq q^2 <1$, donc $b_k$ et $c_k$ convergent par le cas précédent vers $l$.\\
    Si $q<-1, q^2 >1$, donc les suites $b_n$ et $c_n$ divergent par le cas précédent, donc $a_n$ diverge également.
\end{proof}
\begin{example}[Non linéaire]
    $a_0 = $ fixé $\in \R^*$, $a_{n+1} = \frac{1}{2}\left(a_n+\frac{3}{a_n}\right) = g(a_n)$ où $g(x) = \frac{1}{2}\left(x + \frac{1}{3}\right)$. Les candidats pour $l$ sont $x = \sqrt{3}$ ou $x=-\sqrt{3}$.
    \begin{center}
    \begin{tabular}{c|cccc}
    $x$ & $(-\infty,-\sqrt{3})$ & $(-\sqrt{3},0)$ & $(0,\sqrt{3})$ & $(\sqrt{3},+\infty)$ \\ \hline
    $3-x^2$ & $-$ & $+$ & $+$ & $-$ \\
    $x$ & $-$ & $-$ & $+$ & $+$ \\
    $g(x)-x=\dfrac{3-x^2}{2x}$ & $+$ & $-$ & $+$ & $-$ \\
    Comparaison & $g(x)>x$ & $g(x)<x$ & $g(x)>x$ & $g(x)<x$
    \end{tabular}
    \end{center}
    Donc on a \begin{itemize}
        \item si $a_0\in [\sqrt{3}, +\infty[$, on a \[\sqrt{3}\leq g(x)\leq x\] et $a_n\to \sqrt{3}$
        \item si $a_0\in ]0, \sqrt{3}[$, $a_1 = g(a_0)\geq \sqrt{3}$ alors $a_1$ est dans la région précédente, donc $a_n \to \sqrt{3}$
        \item si $a_0 <0, a_n\to -\sqrt{3}$
    \end{itemize}
    Donc on conclut que si $a_0 > 0, a_n\to\sqrt{3}$ et si $a_0 <0, a_n\to -\sqrt{3}$.
\end{example}
\section{Développements limités}
Le but est d'approximer une fonction par un polynôme. 
\begin{definition}
    Soit $f\in C^n(I)$ où $I$ est un intervalle ouvert et $x_0\in I$. Le polynôme de Taylor de $f$ d'ordre $n$ en $x_0$ est l'unique polynôme $p(x)$ de degré $ \leq n$ dont les dérivées en $x_0$ sont les mêmes que celles de $f$ jusqu'à l'ordre $n$.
\end{definition}
\begin{example}
    Soit $f(x) = \sin(x)$, $x_0=0$. On a 
    \begin{itemize}
        \item $p_1(x)=x$ car $\sin(0) = 0 = p_1(0), \sin'(0) = 1 = p_1'(0)$.
        \item $p_2(x)=x$ car $\sin''(0) = 0 = p_2''(0)$.
        \item $p_3(x)=x-\frac{1}{6}x^3$ car $p_3'(x)=-1-\frac{3}{6}x^2, p_3''(x)=\frac{-6}{6}x, p_3'''(x)=-1 = \sin'''(0)$
    \end{itemize}
\end{example}
Ainsi, la formule pour trouver le $p_n$ polynôme est \begin{align*}
    p_n(x)=\sum_{k=0}^{n} a_k(x-x_0)^k
\end{align*} où $a_k = \frac{f^{(k)}(x_0)}{k!}$
\begin{definition}[Développement limité]
    Soit $f:I\to\R$, où $I$ est un intervalle ouvert, $x_0\in I$. Alors, $f$ admet un développement limité d'ordre $n$ en $x_0$ si 
    \begin{align*}
        f(x) = \text{ polynôme de degré $\leq n$ } + \text{ reste } r_n(x)
    \end{align*}
    pour tout $x\in I$ où $r_n(x)$ vérifie
     \begin{itemize}
        \item Pour tout $c > 0$, $|r_n(x)| \leq c|x-x_0|^n$ dans un voisinage de $x_0$.
        \item $\displaystyle\lim_{x\to x_0} \frac{r_n(x)}{(x-x_0)^n} = 0$
        \item $r_n(x) = (x-x_0)^n\varepsilon(x)$, où $\varepsilon : I\to\R$ est telle que $\displaystyle\lim_{x\to x_0}\varepsilon(x)=0$
    \end{itemize}
\end{definition}
\begin{theorem}[Formule de Taylor]
    Soit $f\in C^n(I)$, où $I$ est un intervalle ouvert, $x_0\in I$. Alors $f$ admet un $DL_n$ en $x_0$ donné par \[f(x)=p_n(x)+(x-x_0)^n\varepsilon(x)\]
\end{theorem}
\begin{proof}
    On applique B-H $n$ fois.    
\end{proof}
\begin{remark}
    Soit $f:I\to\R$, $x_0\in I$ ouvert. Alors \begin{itemize}
        \item Si $f$ admet un $DL_n$, il est unique.
        \item Si $f$ est continue en $x_0$, alors $f$ admet un $DL_0$ en $x_0$, de plus l'autre sens en vrai aussi si on suppose que $\varepsilon (x)=0$.
        \item Si $f$ est continue en $x_0$ alors $f$ admet un $DL_1$ en $x_0$ si et seulement si $f$ est dérivable. 
    \end{itemize}
\end{remark}
\begin{example}
    On va voir que $\sin(x)$ admet un développement limité d'ordre $3$ en $x_0 = 0$ donné par $\sin(x) = x - \frac{1}{6}x^3 + x^3\varepsilon(x)$. La seule chose à montrer est 
    \begin{align*}
        &\lim_{x\to 0} \varepsilon(x)=0\\
        \iff& \lim_{x\to 0} \frac{\sin(x)-x+\frac{1}{6}x^2}{x^3}\overset{\text{B-H}}{=} 0
    \end{align*}
    On a donc que le $DL_2$ en $x_0=0$ est \[x+x^2\varepsilon (x)\]
\end{example}
\begin{remark}[Estimation du reste]
    On dit que
    $|r_n(x)|\leq f^{(n+1)}(u)=\frac{|x-x_0|^{n+1}}{n!}$
\end{remark}
\subsection{Applications : Calculs de limites}

\begin{itemize}
    \item Calculons la limite suivante : $\lim_{x\to 0}\frac{\sin(x)-x}{x^3}$
        \begin{align*}
            &=\lim_{x\to 0} \frac{x - \frac{1}{6}x^3+x^3\varepsilon(x)-x}{x^3}\\
            &=\lim_{x\to 0}\left(-\frac{1}{6}+\varepsilon(x) = -\frac{1}{6}\right)
        \end{align*}
        On voit qu'avec un $DL_1$ ca ne marche pas. Donc :
        \begin{align*}
            \lim_{x\to 0}\frac{\sin(x)-x}{x^3} &= \lim_{x\to 0}\frac{x+x\varepsilon(x)-x}{x^3}\\
            &=\lim_{x\to 0}\frac{\varepsilon(x)}{x^2}
        \end{align*}
        Donc il a fallu prendre un développement limité d'ordre plus grand.
    \item $\lim_{x\to 0}\frac{x^2 - 2 + 2\cos(x)}{x^4}$ \begin{align*}
            &=\lim_{x\to 0} \frac{1}{x^4}\left(x^2-2+2\left(1-\frac{x^2}{2}+\frac{x^4}{24}+x^4\varepsilon(x)\right)\right)\\
            &=\lim_{x\to 0}\frac{1}{12+\varepsilon(x)} = \frac{1}{12}
        \end{align*}
    \item $\sum_{n=1}^{\infty}\left(1-\cos\left(\frac{1}{n}\right)\right)$ : \begin{align*}
        =\sum_{n=1}^{\infty}\left(1-\left(1-\frac{\left(\frac{1}{n}\right)^2}{2} + \left(\frac{1}{n}\right)^2 \varepsilon(x)\right)\right)\\
        =\sum_{n=1}^{\infty}\frac{\left(\frac{1}{n}\right)^2}{n}-\left(\frac{1}{n}\right)^2\varepsilon\left(\frac{1}{n}\right)\leq \sum_{n = 1}^{\infty}\left(\frac{1}{n}\right)^2
    \end{align*}
    Comme $c$ est supérieur à $0$, cette série converge et la série originale aussi, par comparaison.
\end{itemize}
\begin{example}[Calculs de $DL$]
    Tant qu'on écrit le rest, on peut tout faire.
\end{example}
\begin{example}[Le $DL_2$ en $x_0 = 0$ de $f(x)=\sin(x)\cos(x)$] 
    On utilise le $DL_2$ de $\sin(x)$ et le $DL_1$ de $\cos(x)$. Donc \begin{align*}
        \sin(x)\cos(x)&=\left(x+x^2\varepsilon(x)\right)\left(1+x\varepsilon(x)\right)\\
        &=x+x^2\varepsilon(x)+x^2\varepsilon(x)+x^3\varepsilon(x)\varepsilon(x)\\
        &=x+x^2\left(\underbrace{\varepsilon(x) + \varepsilon(x)+ x\varepsilon(x)\varepsilon(x)}_{\to 0}\right)\\
        &=x+x^2\varepsilon(x)
    \end{align*}
\end{example}
\begin{example}Le $DL_1$ en $x_0$ de $f(x)=e^{\cos(x)}$. On utiliser les $DL_1$ de $\cos(x)$ et $e^x$. \begin{align*}
    e^{\cos(x)} &=1 + \cos(x)+ \cos(x)\varepsilon\left(\underbrace{\cos(x)}_{\to 1}\right)
\end{align*}
On voit que $\varepsilon(x)$ ne tend pas vers $0$. \\
\textbf{Solution : } Méthode (i) : il faut réécrire l'expression pour avoit quelque chose qui tend vers $0$. Donc : \begin{align*}
    e^{\cos(x)} &= e ^{1+x\varepsilon(x)} = e\cdot e^{c\varepsilon(x)}\\
    &= e\left(1+x\varepsilon(x)+x\varepsilon(x)\varepsilon(x\varepsilon(x))\right)\\
    &= e + x\left(e\varepsilon(x) + e\varepsilon(x)\cdot\varepsilon(x\varepsilon(x))\right)
\end{align*}
Methode (ii) : Combiner le $DL_1$ en $x_0 = 0$ et de $\cos(x)$ avec le $DL_1$ en $x_0 = \cos(0) = 1$ de $e^x$. Donc : \begin{align*}
    e^x&=e+e(x-1)+(x-1)\underbrace{\tilde{\varepsilon}(x)}_{\to 0 \text{ lorsque } x\to1}\\
    \implies & e^{\cos(x)} = e + e\left(\cos(x)-1\right)+\left(\cos(x)-1\right)\tilde{\varepsilon}(x)\\
    &= e+ e(x\varepsilon(x))+(x\varepsilon(x))\tilde{\varepsilon}(\cos(x))\\
    &=e+x\left(e\varepsilon(x)\right) + \varepsilon(x)\tilde{\varepsilon}(\cos(x))
\end{align*}
\end{example}

\subsection{Séries de Taylor}
Si $f \in C^n(I)$, avec $I$ un intervalle ouvert, $x_0\in I$, on a \[f(x)=\sum_{k=0}^{n}\frac{f^{(n)}(x_0)}{k!} (x-x_0)^k + r_n(x)\]
Question : si $f\in C^\infty(I)$, a-t-on $f(x)=\sum_{k=0}^{\infty}\frac{f^{(n)}(x_0)}{k!} (x-x_0)^k$ ? Quand on a $n\to \infty$, il faut que :
\begin{enumerate}
    \item La série converge
    \item Le reste $r_n(x) \longrightarrow 0$ lorsque $n\to \infty$
\end{enumerate}
\begin{definition}
    Pour $f\in C^\infty(I)$ avec $I$ un intervalle ouvert, $x_0\in I$, la série de Taylor de $f$ centrée en $x_0$ est la série \[\sum_{k = 0}^{\infty}\frac{f^{(n)}(x_0)}{k!}(x-x_0)^k\]
\end{definition}
\begin{remark}
    \noindent\begin{itemize}
        \item Cette série est définie quelque soit $f\in C^\infty(I)$ (même si $r_n(x) \nrightarrow  0$)
        \item Si x$_0 = 0$, elle s'appelle aussi série de Maclaurin.
        \item C'est une série entière ! (cf. Chap. 3.3 $\rightsquigarrow $ centre $=x_0$, rayon de convergence pas connu)
    \end{itemize}
\end{remark}
\begin{example}[1]
    $f(x) = e^x\in C^\infty(\R), x_0 = 0$. Le $DL_n$ de $f$ donne \[e^x =\sum_{k = 0}^{\infty}\frac{x^k}{k!}+x^n\varepsilon(x)\] 
    Donc la série de Taylor (en $x_0 = 0$) est donc $\sum_{k = 0}^{\infty}\frac{x^k}{k!}$. Cette série converge pour tout $x\in\R$. Il reste à voir que $\lim_{x\to \infty}r_n(x) = 0$. On a :
    \[|r_n(x)| \leq f^{(n+1)} (u) \frac{|x|^{n+1}}{n!} = e^u\frac{|x|^{n+1}}{n !} \leq \frac{e ^{|x|} |x|^{n+1}}{n!} \longrightarrow 0 \text{ lorsque } n\to \infty \]
    Ainsi la limite du ret et bien $0$. Donc $e^x$ est égale à sa série de Taylor en $x_0 = 0$ pour tout $x\in\R$ :
    \[e^x = \sum_{k = 0}^{\infty}\frac{x^k}{k!}\] 
    \begin{remark}
        En fait, $e^x = \sum_{k = 0}^{\infty}\frac{e^{x_0}}{k!}(x-x_0)^n$
    \end{remark}
\end{example}

\begin{example}[2]
    Cela marche aussi pour $\sin$, $\cos$, $\sinh$, $\cosh$ ! 
    \begin{align*}
        \implies & \sin(x) = \sum_{k = 0}^{\infty}\frac{(-1)^k}{(2k+1)!} x^{2k+1}\\
        \implies & \cos(x) = \sum_{k = 0}^{\infty}\frac{(-1)^k}{(2k)!} x^{2k}
    \end{align*}
    pour tout $x\in\R$.
    \begin{remark}
        Cela donne enfin une raison pour la formule $e^{ix} = \cos(x) + i\sin(x)$. En effet \begin{align*}
            e^{ix}&=\sum_{k = 0}^{\infty}\frac{(ix)^k}{k!} = \sum_{k = 0}^{\infty}\frac{(ix)^2}{(2k)!} + \sum_{k = 0}^{\infty} \frac{(ix)^{2k+1}}{(2k+1)!}\\
            &= \sum_{k = 0 }^{\infty}\frac{(-1)^{k}}{(2k)!} x^{2k} + i \sum_{k=0}^{\infty}\frac{(-1)^k}{(2k+1)!}x^{2k+1}\\
            &= \cos(x)+i\sin(x)
        \end{align*}
    \end{remark}
\end{example}
\begin{property}[Dériviées de série entières]
    Si $f(x) = \sum_{k = 0}^{\infty}b_k(x-x_0)^k$, alors $f'(x) = \sum_{k = 0}^{\infty}k\cdot b_k(x-x_0)^{k-1} = \sum_{k = 0}^{\infty}(k+1)b_{k+1}(x-x_0)$ et les rayons de convergences sont les mêmes !
    Concéquences de la porp :
    \begin{itemize}
        \item On peut définir \[e^x = \exp(x)=\sum_{k=0}^{\infty}\frac{x^k}{k!}\quad \forall x\in\R\]
            En effet, $\exp(0)=\frac{0^0}{0!} =1$ et \[\exp'(x)=\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!} \overset{j=k-1}{=} \sum_{j=0}^{\infty}\frac{x^j}{j!} = \exp(x)\]
            C'est donc l'unique fonction $f$ telle que $f'(x)=f(x)$ et $f(0)=1$.
        \item Si $f(x)$ est déjà une série entière, i.e. $f(x)=\sum_{k = 0}^{\infty}b_k(x-x_0)^k$, on dérive :
            \[f(x_0) = b_0, f'(x) = \sum_{k = 1}^{\infty}b_k(x-x_0)^{k-1} \implies f'(x_0) = b_1\]
            \[f^{(k)}(x)=b_k\cdot k!\implies b_k = \frac{f^{(k)}(x_0)}{k!}\]
    \end{itemize}
\end{property}
\begin{example}[3]
    $f(x) = \frac{1}{1-x} \in C^\infty(]-1, 1[)$. On a vu que \[\frac{1}{1-x} = \sum_{k = 0}^{\infty}x^k = \sum_{k = 0}^{\infty} 1(x-0)^k\quad \text{ si } |x|<1\]
    Ainsi \[f(x)= \frac{1}{1-x} = \text{ série de Taylor en $x_0 = 0$ } =\sum_{k = 0}^{\infty}x^k \]
    Attention : pour $x\notin ]-1, 1[$, la série diverge. Par example :
    \[f(-10)=\frac{1}{11}\neq\sum_{k = 0}^{\infty}(-10)^k\]
\end{example}
\begin{example}[4]
    $f(x)=\log(1+x) = \sum_{k = 1}^{n}\frac{(-1)^{k-1}}{k}x^k + x^n \varepsilon(x)$. Série de Taylor ?? $\sum_{k = 1}^{\infty}\frac{(-1)^{k-1}}{k}x^k$. On calcule $f(x)-\text{ série de Taylor }$ : 
    \begin{align*}
       f(x) - \text{ série de Taylor }&= \frac{1}{1+x} - \sum_{k = 1}^{\infty}\frac{(-1)^{k-1}}{k}kx^{k-1}\\
       &= \frac{1}{1-(-x)} - \sum_{k = 0}^{\infty}(-x)^k = 0 quad \text{ cf. exemple précédent }\\
       &\implies \log(1+x) - \sum_{k = 1}^{\infty}\frac{(-1)^{k-1}}{k}x^k = C = 0\\
       &\implies \log(1+x) = \sum_{k = 1}^{\infty}\frac{(-1)^{k-1}}{k}x^k
    \end{align*}
    pour $x\in ]-1, 1[$. En $x=1$, on obtient, par prolongement par continuité que : \[\log(2) = \sum_{k = 1}^{\infty}\frac{(-1)^{k-1}}{k}\]
\end{example}
\begin{example}[5 (Contre example)]
    On pose $f(x) = \begin{cases}
        e^{-\frac{1}{x^2}} & x\neq 0\\
        0 & x=0
    \end{cases}$. On vérifie qu'elle est continue en $0$ : 
    \[\lim_{x\to 0}f(x) = e^{-\infty} = 0 = f(0)\] Donc $f$ est continue en $0$. De plus, si $x\neq 0$, $f'(x) = e^{\frac{-1}{x^2}}\frac{2}{x^3}$ et on calcule $\lim_{x\to 0}f'(x)=\lim_{x\to 0}e^{\frac{-1}{x^2}} \frac{2}{x^3}$ :
    \begin{align*}
        &= \lim_{y\to \pm\infty}e^{-y^2}2y^3\\
        &=\lim_{y\to \pm\infty}\frac{2y^3}{e^{y^2}} = 0\\
        \implies & f'(0) = 0\\
        \implies & f'(x) = \begin{cases}
            e^{\frac{-1}{x^2}}\frac{2}{x^3} & x\neq 0\\
            0 & x=0
        \end{cases}
    \end{align*}
    De manière similaire, on montre que \[f^{(k)}(x) = \begin{cases}
        P_k\left(\frac{1}{x}\right)e^{\frac{-1}{x^2}} & x\neq 0\\
        0 & x=0
    \end{cases}\] où $P_k$ est un polynôme. Donc $f\in C^\infty(\R)$ et $f^{(k+1)}(0)=0$ pour tout $k\in\N$. Ainsi, la série de Taylor de $f$ en $x_0 = 0$ est nulle : \[ \sum_{k = 0}^{\infty}\frac{f^{(k)}(0)}{k!}x^k = 0\]
    Cependant, $f(x) \neq 0$ pour $x\neq 0$. Donc $f$ n'est pas égale à sa série de Taylor en $x_0 = 0$. La raison est que le $DL$ de $f$ est \[f(x) = 0 + x^n\varepsilon(x) = r_n(x)=e^{\frac{-1}{x^2}}\] 
\end{example}
\begin{remark}
    Prenons $\sin(x)$ et $\sin(x) + e^{\frac{-1}{x^2}}$. Ces deux fonctions ont la même série de Taylor en $x_0 = 0$, mais seule $\sin(x)$ est égale à sa série de Taylor ! 
\end{remark}